#ifndef _H_TIRAMISU_CORE_
#define _H_TIRAMISU_CORE_

#include <isl/set.h>
#include <isl/map.h>
#include <isl/union_map.h>
#include <isl/union_set.h>
#include <isl/ast_build.h>
#include <isl/schedule.h>
#include <isl/schedule_node.h>
#include <isl/space.h>

#include <map>
#include <string.h>
#include <stdint.h>

#include <Halide.h>
#include <tiramisu/debug.h>
#include <tiramisu/expr.h>
#include <tiramisu/type.h>

namespace tiramisu
{

class function;
class computation;
class buffer;
class constant;

/**
  * Add a dimension to the range of a map in the specified position.
  * Assume that the name of the new dimension is equal to the name of the corresponding
  * dimension in the domain of the map.
  * Add a constraint that indicates that the added dim is equal to a constant.
  */
isl_map *isl_map_add_dim_and_eq_constraint(isl_map *map, int dim_pos, int constant);

struct HalideCodegenOutput {
    std::map<std::string, tiramisu::computation *> computation_list;
    std::map<std::string, tiramisu::constant *> constant_list;
    std::map<std::string, tiramisu::buffer *> output_buffers;

    HalideCodegenOutput(const std::map<std::string, tiramisu::computation *> &computations,
                        const std::map<std::string, tiramisu::constant *> &constants,
                        const std::map<std::string, tiramisu::buffer *> &buffers)
        : computation_list(computations), constant_list(constants), output_buffers(buffers) {}
};

HalideCodegenOutput halide_pipeline_to_tiramisu_function(
    Halide::Internal::Stmt s,
    const std::vector<Halide::Internal::Function> &outputs,
    const std::map<std::string, Halide::Internal::Function> &env,
    const std::map<std::string, std::vector<int32_t>> &output_buffers_size,
    tiramisu::function *func);

void halide_pipeline_to_c(
    Halide::Internal::Stmt s,
    const std::vector<Halide::Internal::Function> &outputs,
    const std::map<std::string, Halide::Internal::Function> &env,
    const std::map<std::string, std::vector<int32_t>> &output_buffers_size,
    const std::string &func);


/**
  * A class to represent functions.  A function is composed of
  * computations (of type tiramisu::computation).
  */
class function
{
private:
    /**
      * The name of the function.
      * Function names should not start with _ (an underscore).
      * Names starting with _ are reserved names.
      */
    std::string name;

    /**
      * Function arguments.  These are the buffers or scalars that are
      * passed to the function.
      */
    std::vector<tiramisu::buffer *> function_arguments;

    /**
      * A vector representing the invariants of the function (symbolic
      * constants or variables that are invariant to the function i.e.
      * do not change their value during the execution of the function).
      */
    std::vector<tiramisu::constant> invariants;

    /**
      * An ISL context associate with the function.
      */
    isl_ctx *ctx;

    /**
      * An ISL AST representation of the function.
      * The ISL AST is generated by calling gen_isl_ast().
      */
    isl_ast_node *ast;

    /**
      * A vector representing the parallel dimensions around
      * the computations of the function.
      * A parallel dimension is identified using the pair
      * <computation_name, level>, for example the pair
      * <S0, 0> indicates that the loop with level 0
      * (i.e. the outermost loop) around the computation S0
      * should be parallelized.
      */
    std::vector<std::pair<std::string, int>> parallel_dimensions;

    /**
      * A vector representing the vectorized dimensions around
      * the computations of the function.
      * A vector dimension is identified using the pair
      * <computation_name, level>, for example the pair
      * <S0, 0> indicates that the loop with level 0
      * (i.e. the outermost loop) around the computation S0
      * should be vectorized.
      */
    std::vector<std::pair<std::string, int>> vector_dimensions;

    /**
      * A vector representing the GPU dimensions around
      * the computations of the function.
      * GPU dimensions are dimensions that should be mapped
      * to parallel GPU threads.
      * GPU dimensions are identified using the tuple
      * <computation_name, level0, level1>, for example the tuple
      * <S0, 0, 1> indicates that the loops with levels 0 and 1
      * (i.e. the two outermost loops) around the computation S0
      * should be mapped to GPU.
      * Level1 must be the level following level0, i.e.
      * level1 == level0 + 1
      */
    std::vector<std::pair<std::string, std::pair<int, int>>> gpu_dimensions;

    /**
      * A vector representing the dimensions that should be unrolled
      * around the computations of the function.
      * Unrolled dimensions are identified using the tuple
      * <computation_name, level0>, for example the tuple
      * <S0, 1> indicates that the loops with level 1
      * around the computation S0 should be unrolled.
      */
    std::vector<std::pair<std::string, int>> unroll_dimensions;

    /**
      * Body of the function (a vector of computations).
      * The order of the computations in the vector does not have any
      * effect on the actual order of execution of the computations.
      * The order of execution of computations is specified through the
      * schedule.
      */
    std::vector<computation *> body;

    /**
      * A Halide statement that represents the whole function.
      * This value stored in halide_stmt is generated by the code generator
      * and is empty before calling the code generator.
      */
    Halide::Internal::Stmt *halide_stmt;

    /**
      * A map representing the buffers of the function.  Some of these
      * buffers are passed to the function as arguments and some are
      * declared and allocated within the function itself.
      */
    std::map<std::string, tiramisu::buffer *> buffers_list;

    /**
     * The context set of the function.  i.e. a set representing the
     * constraints over the parameters.
     * The parameters of a function are the function invariants (constants).
     */
    isl_set *context_set;

    /**
     * The names of the iterators.
     */
    std::vector<std::string> iterator_names;

    /**
     * This functions iterates over the schedules of the function (the schedule
     * of each computation in the function) and computes the maximal dimension
     * among the dimensions of the ranges of all the schedules.
     */
    int get_max_schedules_range_dim() const;

    /**
      * This function first computes the identity schedules,
      * then it computes the maximal dimension among the dimensions
      * of the ranges of all the identity schedules.
      */
    int get_max_identity_schedules_range_dim() const;

    /**
      * This function iterates over the computations of the function.
      * It modifies the identity schedule of each computation in order to
      * make all the identity schedules have the same number of dimensions
      * in their ranges.
      * This is done by adding dimensions equal to 0 to the range of each
      * identity schedule that does not have enough dimensions.
      */
    isl_union_map *get_aligned_identity_schedules() const;

public:

    /**
     * Construct a function with the name \p name.
     * Function names should not start with _ (an underscore).
     * Names starting with _ are reserved names.
     */
    function(std::string name);

    /**
      * Get the arguments of the function.
      */
    const std::vector<tiramisu::buffer *> &get_arguments() const;

    /**
      * Return a map that represents the buffers of the function.
      * The buffers of the function are buffers that are either passed
      * to the function as arguments or are buffers that are declared
      * and allocated within the function itself.
      * The names of the buffers are used as a key for the map.
      */
    const std::map<std::string, tiramisu::buffer *> &get_buffers() const;

    /**
      * Return a vector of the computations of the function.
      * The order of the computations in the vector does not have any
      * effect on the actual order of execution of the computations.
      * The order of execution of computations is specified through the
      * schedule.
      */
    const std::vector<computation *> get_computations() const;

    /**
      * Return the computation of the function that has
      * the name \p str.
      */
    computation *get_computation_by_name(std::string str) const;

    /**
      * Return a set that represents the parameters of the function
      * (an ISL set that represents the parameters and constraints over
      * the parameters of the functions,  a parameter is an invariant
      * of the function). This set is also known as the context of
      * the program.
      * An example of a context set is the following:
      *          "[N,M]->{: M>0 and N>0}"
      * This context set indicates that the two parameters N and M
      * are strictly positive.
      */
     isl_set *get_parameter_set() const;


     /**
      * Return the context set of this function.
      * A context is an ISL set that represents constraints over
      * the parameters of the functions (a parameter is an invariant
      * variable for the function).
      * An example of a context set is the following:
      *          "[N,M]->{: M>0 and N>0}"
      * This context set indicates that the two parameters N and M
      * are strictly positive.
      */
     isl_set *get_context_set();

     /**
       * Return the isl_ctx associated with this function.
       * This is an ISL specific object required when calling certain
       * ISL functions.  It does not represent the set of parameters
       * of the function (which should be retrieved by calling
       * get_parameter_set()).
       */
     isl_ctx *get_ctx() const;

    /**
      * Get the name of the function.
      */
    const std::string &get_name() const;

    /**
      * Return a vector representing the invariants of the function
      * (symbolic constants or variables that are invariant to the
      * function i.e. do not change their value during the execution
      * of the function).
      */
    const std::vector<tiramisu::constant> &get_invariants() const;

    /**
      * Return the Halide statement that represents the whole
      * function.
      * The Halide statement is generated by the code generator.
      * This function should not be called before calling the code
      * generator.
      */
    Halide::Internal::Stmt get_halide_stmt() const;

    /**
      * Return the union of all the iteration domains
      * of the computations of the function.
      */
    isl_union_set *get_iteration_domain() const;

    /**
      * Return the union of all the schedules
      * of the computations of the function.
      */
    isl_union_map *get_schedule() const;

    /**
      * Return an ISL AST that represents this function.
      * This function itself does not generate the ISL AST, it just
      * returns it if it already exists.
      * The function gen_isl_ast() should be called before calling
      * this function.
      */
    isl_ast_node *get_isl_ast() const;

    /**
      * Return the union of time-processor domains of each
      * computation in the function.
      * In the time-processor representation, the logical time of
      * execution and the processor where the computation will be
      * executed are both specified.
      */
    isl_union_set *get_time_processor_domain() const;

    /**
    * Get the iterator names of the function.
    */
    const std::vector<std::string>& get_iterator_names() const;

    /**
       * Return a string representing the name of the GPU iterator at
       * dimension \p lev0.
       * This function only returns a non-empty string if the
       * computation \p comp is mapped to GPU at the dimension \p lev0.
       */
     std::string get_gpu_iterator(std::string comp, int lev0) const;

     /**
       * Return true if the computation \p comp should be parallelized
       * at the loop level \p lev.
       */
     bool should_parallelize(std::string comp, int lev) const;

     /**
       * Return true if the computation \p comp should be vectorized
       * at the loop level \p lev.
       */
     bool should_vectorize(std::string comp, int lev) const;

     /**
       * Return true if the computation \p comp should be unrolled
       * at the loop level \p lev.
       */
     bool should_unroll(std::string comp, int lev) const;

     /**
       * Return true if the computation \p comp should be mapped to GPU
       * at the loop levels \p lev0.
       */
     bool should_map_to_gpu(std::string comp, int lev0) const;

    /**
      * Add an invariant to the function.
      */
    void add_invariant(tiramisu::constant param);

    /**
      * Add a buffer to the function.
      * The buffers of the function are either:
      * - buffers passed to the function as arguments, or
      * - buffers that are declared and allocated within the function
      * itself.
      * The first element of the pair is the name of the buffer (it is
      * used as a key), the second element of the pair is a pointer
      * to the buffer.
      */
    void add_buffer(std::pair<std::string, tiramisu::buffer *> buf);

    /**
      * Add a computation to the function.  The order in which
      * computations are added to the function is not important.
      * The order of execution is specified using the schedule.
      * This doesn't allow computations with duplicate names.
      */
    void add_computation(computation *cpt);

    /**
      * Set the arguments of the function.
      * The arguments of the function are provided as a vector of
      * pointers to buffers. Each buffer represents an argument
      * to the function.
      * During code generation, the arguments in the vector will become
      * the arguments of the generated function (with the order of their
      * appearance in the vector).
      */
    void set_arguments(std::vector<tiramisu::buffer *> buffer_vec);

    /**
     * Set the context of the function. A context is an ISL set that
     * represents constraints over the parameters of the functions
     * (a parameter is an invariant variable for the function).
     * An example of a context set is the following:
     *          "[N,M]->{: M>0 and N>0}"
     * This context set indicates that the two parameters N and M
     * are strictly positive.
     */
    // @{
    void set_context_set(std::string context_str);
    void set_context_set(isl_set *context);
    // @}

    /**
      * Intersect the set provided as input with the context of the function.
      * A context is an ISL set that represents constraints over the parameters
      * of the functions (a parameter is an invariant variable for the function).
      * An example of a context set is the following:
      *          "[N,M]->{: M>0 and N>0}"
      * This context set indicates that the two parameters N and M
      * are strictly positive.
      * The input set should have the same space as the context set.
      */
    void add_context_constraints(std::string new_context_str);

    /**
      * This functions applies to the schedule of each computation
      * in the function.  It makes the dimensions of the ranges of
      * all the schedules equal.  This is done by adding dimensions
      * equal to 0 to the range of schedules.
      * This function is called automatically when gen_isl_ast()
      * or gen_time_processor_domain() are called.
      */
    void align_schedules();

    /**
      * Tag the dimension \p dim of the computation \p computation_name to
      * be parallelized.
      * The dimension 0 represents the outermost loop level (it
      * corresponds to the leftmost dimension in the iteration space).
      */
    void add_parallel_dimension(std::string computation_name, int vec_dim);

    /**
      * Tag the dimension \p dim of the computation \p computation_name to
      * be vectorized.
      * The dimension 0 represents the outermost loop level (it
      * corresponds to the leftmost dimension in the iteration space).
      */
    void add_vector_dimension(std::string computation_name, int vec_dim);

    /**
      * Tag the dimensions \p dim0 and \p dim1 of the computation
      * \p computation_name to be mapped to GPU.
      * The dimension 0 represents the outermost loop level (it
      * corresponds to the leftmost dimension in the iteration space).
      */
     void add_gpu_dimensions(std::string computation_name, int dim0, int dim1);

     /**
       * Tag the loop level \p L of the computation
       * \p computation_name to be unrolled.
       * The dimension 0 represents the outermost loop level (it
       * corresponds to the leftmost dimension in the iteration space).
       */
     void add_unroll_dimension(std::string stmt_name, int L);

    /**
      * Set the iterator names of the function.
      * This function overrides any previously set iterator names.
      */
    void set_iterator_names(const std::vector<std::string>& iteratorNames);

    /**
      * Add an iterator to the function.
      */
    void add_iterator_name(const std::string iteratorName);

    /**
       * Generate an object file that contains the compiled function.
       * This function relies on Halide to generate the object file and
       * thus requires Halide objects as inputs.
       * \p obj_file_name indicates the name of the generated file.
       * \p os indicates the target operating system (Halide::Target::OS).
       * \p arch indicates the architecture of the target (the instruction set).
       * \p bits indicate the bit-width of the target machine.
       *    must be 0 for unknown, or 32 or 64.
       * For a full list of supported values for \p os and \p arch please
       * check the documentation of Halide::Target
       * (http://halide-lang.org/docs/struct_halide_1_1_target.html).
       * If the machine parameters are not supplied, Halide detects
       * the parameters of the host machine automatically.
       */
     // @{
     void gen_halide_obj(std::string obj_file_name, Halide::Target::OS os,
                         Halide::Target::Arch arch, int bits) const;

     void gen_halide_obj(std::string obj_file_name) const;
     // @}

     /**
       * Generate C code on stdout.
       * Currently C code code generation is very basic and does not
       * support many features compared to the Halide code generator.
       * Use this for debugging only.
       */
     void gen_c_code() const;

     /**
       * Generate an isl AST that represents the function.
       */
     void gen_isl_ast();

     /**
       * Generate a Halide stmt that represents the function.
       */
     void gen_halide_stmt();

     /**
       * Generate the time-processor domain of the function.
       * In this representation, the logical time of execution and the
       * processor where the computation will be executed are both
       * specified.
       */
     void gen_time_processor_domain();

    /**
      * Dump the iteration domain of the function.
      * This is mainly useful for debugging.
      */
    void dump_iteration_domain() const;

    /**
      * Dump the schedules of the computations of the function.
      * This is mainly useful for debugging.
      * The schedule is a relation between the iteration space and a
      * time space.  The relation provides a logical date of execution for
      * each point in the iteration space.
      * The schedule needs first to be set before calling this function.
      */
    void dump_schedule() const;

    /**
       * Dump (on stdout) the time processor domain of the function.
       * The time-processor domain should be generated before calling
       * this function (gen_time_processor_domain()).
       * This is mainly useful for debugging.
       */
     void dump_time_processor_domain() const;

     /**
       * Dump a Halide stmt that represents the function.
       * gen_halide_stmt should be called before calling this function.
       */
     void dump_halide_stmt() const;

    /**
      * Dump the function on standard output (dump most of the fields of
      * the function class).
      * This is mainly useful for debugging.
      * If \p exhaustive is set to true, all the fields of the function
      * class are printed.  This is useful for finding potential
      * initialization problems.
      */
    void dump(bool exhaustive) const;
};


/**
  * A class that represents buffers.
  * Buffers have two use cases:
  * - used to store the results of computations, and
  * - used to represent input arguments to functions.
  */
class buffer
{
private:
    /**
      * The name of the buffer.
      * Buffer names should not start with _ (an underscore).
      * Names starting with _ are reserved names.
      */
    std::string name;

    /**
      * The number of dimensions of the buffer.
      */
    int nb_dims;

    /**
      * The sizes of the dimensions of the buffer.  Assuming the following
      * buffer: buf[N0][N1][N2].  The first vector element represents the
      * size of rightmost dimension of the buffer (i.e. N2), the second
      * vector element is N1, and the last vector element is N0.
      */
    std::vector<tiramisu::expr> dim_sizes;

    /**
      * The type of the elements of the buffer.
      */
    tiramisu::primitive_t type;

    /**
      * Buffer data.
      */
    uint8_t *data;

    /**
      * The tiramisu function where this buffer is declared or where the
      * buffer is an argument.
      */
    tiramisu::function *fct;

    /**
     * Type of the argument (if the buffer is an argument):
     * Three possible types:
     *  - a_input: for inputs of the function,
     *  - a_output: for outputs of the function,
     *  - a_temporary: for buffers used as temporary buffers within
     *  the function (any temporary buffer is allocated automatically by
     *  the Tiramisu runtime at the entry of the function and is
     *  deallocated at the exit of the function).
     */
    tiramisu::argument_t argtype;

public:
    /**
      * Create a tiramisu buffer.
      * Buffers have two use cases:
      * - used to store the results of computations, and
      * - used to represent input arguments to functions.
      *
      * \p name is the name of the buffer.
      * \p nb_dims is the number of dimensions of the buffer.
      * A scalar is a one dimensional buffer that has a size of one
      * element.
      * \p dim_sizes is a vector of integers that represent the size
      * of each dimension in the buffer.  The first vector element
      * represents the rightmost array dimension, while the last vector
      * element represents the leftmost array dimension.
      * For example, in the buffer buf[N0][N1][N2], the first element
      * in the vector \p dim_sizes represents the size of rightmost
      * dimension of the buffer (i.e. N2), the second vector element
      * is N1, and the last vector element is N0.
      * Buffer dimensions in Tiramisu have the same semantics as in
      * C/C++.
      * \p type is the type of the elements of the buffer.
      * It must be a primitive type (i.e. p_uint8, p_uint16, ...).
      * Possible types are declared in tiramisu::primitive_t (type.h).
      * \p data is the data stored in the buffer.  This is useful
      * if an already allocated buffer is passed to Tiramisu.
      * \p fct is a pointer to a Tiramisu function where the buffer is
      * declared or used.
      * \p is_argument indicates whether the buffer is passed to the
      * function as an argument.  All the buffers passed as arguments
      * to the function should be allocated by the user outside the
      * function.  Buffers that are not passed to the function as
      * arguments are allocated automatically at the beginning of
      * the function and deallocated at the end of the function.
      * They are called temporary buffers (of type a_temporary).
      * Temporary buffers cannot be used outside the function
      * in which they were allocated.
      *
      * Buffer names should not start with _ (an underscore).
      * Names starting with _ are reserved names.
      */
    buffer(std::string name, int nb_dims, std::vector<tiramisu::expr> dim_sizes,
           tiramisu::primitive_t type, uint8_t *data,
           tiramisu::argument_t argt, tiramisu::function *fct);

    /**
      * Return the type of the argument (if the buffer is an argument).
      * Three possible types:
      *  - a_input: for inputs of the function,
      *  - a_output: for outputs of the function,
      *  - a_temporary: for buffers used as temporary buffers within
      *  the function (any temporary buffer is allocated automatically by
      *  the Tiramisu runtime at the entry of the function and is
      *  deallocated at the exit of the function).
      */
    tiramisu::argument_t get_argument_type() const;

    /**
     * Return a pointer to the data stored within the buffer.
     */
    uint8_t *get_data();

    /**
      * Return the name of the buffer.
      */
    const std::string &get_name() const;

    /**
      * Get the number of dimensions of the buffer.
      */
    int get_n_dims() const;

    /**
    * Return the type of the elements of the buffer.
    */
    tiramisu::primitive_t get_elements_type() const;

    /**
      * Return the sizes of the dimensions of the buffer.
      * Assuming the following buffer: buf[N0][N1][N2].  The first
      * vector element represents the size of rightmost dimension
      * of the buffer (i.e. N2), the second vector element is N1,
      * and the last vector element is N0.
      */
    const std::vector<tiramisu::expr> &get_dim_sizes() const;

    /**
      * Dump the function on standard output (dump most of the fields of
      * the buffer class).
      * This is mainly useful for debugging.
      * If \p exhaustive is set to true, all the fields of the buffer
      * class are printed.  This is useful to find potential initialization
      * problems.
      */
    void dump(bool exhaustive) const;
};



/**
  * A class that represents computations.
  * A computation is an expression associated with an iteration domain.
  * A computation indicates what needs to be computed (the expression
  * that should be computed).
  * A computation has three representations:
  * - Level 1: this level specifies "what" should be computed but does
  *   not specify "when" (order) and "where" (on which processor) each
  *   expression should be computed.
  *   This level also does not specify where computations should be stored
  *   in memory and in which data layout.
  * - Level 2: this level specifies "what" should be computed, "when", i.e.
  *   the order in which the computation should be executed with regard to
  *   the other computations. And "where" each computation should be
  *   computed (i.e., on which processor).
  *   This level still does not specify where computations should be stored
  *   in memory and their data layout.
  * - Level 3: this level is similar to Level 2 but it specifies where
  *   computations should be stored in memory and the data layout.
  */
class computation {
private:

    /**
      * An ISL context associate with the function.
      */
    isl_ctx *ctx;

    /**
      * Iteration domain of the computation.
      * In this representation, the order of execution of computations
      * is not specified, the computations are also not mapped to memory.
     */
    isl_set *iteration_domain;

    /**
      * Time-processor domain of the computation.
      * In this representation, the logical time of execution and the
      * processor where the computation will be executed are both
      * specified.
      */
    isl_set *time_processor_domain;

    /**
      * Schedule of the computation.
      */
    isl_map *schedule;

    /**
      * The function where this computation is declared.
      */
    tiramisu::function *function;

    /**
      * Halide statement that assigns the computation to a buffer location.
      */
    Halide::Internal::Stmt stmt;

    /**
      * Access function.  A map indicating how each computation should be stored
      * in memory.  It indicates in which buffer the computation should be stored
      * and which element of the buffer exactly it should be stored.
      */
    isl_map *access;

    /**
      * An isl_ast_expr representing the index of the array where the computation
      * will be stored.  This index is computed after the scheduling is done.
      */
    std::vector<isl_ast_expr *> index_expr;

    /**
     * Data type: the type of the value returned by the computation.
     */
    tiramisu::primitive_t data_type;

    /**
     * Update the name of the domain name of the schedule of a let statement.
     * If the computation is a let statement, and if the name of
     * the domain of map does not start with the LET_STMT_PREFIX,
     * add the LET_STMT_PREFIX to the name.
     * We use the LET_STMT_PREFIX to identify the computations that
     * represent a let statement.
     */
    isl_map* update_let_stmt_schedule_domain_name(isl_map* map);

    /**
     * A logical time that indicates the relative order of this computation
     * compared to other computations.
     * This should only be used by the .after() function and should not be
     * used directly by users.
     */
    unsigned long relative_order;

    /**
     * Does this computation represent a let statement ?
     *
     * Let statements should be treated differently:
     * - During Halide code generation a Halide let statement should be
     * created instead of an assignment statement.
     * - A let statement does not have/need an access function because
     * it writes directly to a scalar.
     * - When targeting Halide, let statements should be created after
     * their body is created, because the body is an argument needed
     * for the creation of the let statement.
     */
    bool _is_let_stmt;

    /**
     * TODO: use buffers directly from computations, no need to have
     * bindings.
     *
     * \p schedule_this_computation should be set to true when the computation
     * should be scheduled and when code for the computation should be generated
     * during code generation.
     * It should be set to false when the computation is used to represent a
     * buffer (i.e., the computation is used only as a binding to a buffer).
     * In this case, the computation is not scheduled and no code for the
     * computation is generated.
     */
    bool schedule_this_computation;

    /**
      * The name of this computation.
      * Computation names should not start with _ (an underscore).
      * Names starting with _ are reserved names.
      */
    std::string name;

    /**
      * An expression representing the computation
      * ("what" should be computed).
      */
    tiramisu::expr expression;

    /**
     * Separate the iteration domain into two iteration domains using
     * the constant \p C.
     * Let us assume that the dimension \p dim of the iteration domain
     * is called i.  The iteration domain is separated into two domains
     * using the hyperplane (i = C). That means, two copies of the
     * iteration domain are created, the constraint (i<=C) is added to
     * the first while the constrain (i>C) is added to the second.
     *
     * Let us assume the following iteration domain
     *
     *   {S0[i,j]: 0<=i<N and 0<=j<N}
     *
     * To separate this iteration domain by the hyperplane j=M, one should
     * call
     *
     *   S0.separate(1, tiramisu::expr("M"))
     *
     * This will result in the creation of two iteration domains
     *
     * {S0[i,j]: 0<=i<N and 0<=j<M} and {_S0[i,j]: 0<=i<N and M<=j<N}
     *
     */
    void separate(int dim, tiramisu::constant &C);

    /**
     * Set the iteration domain of the computation
     */
    void set_iteration_domain(isl_set *domain);
        tiramisu::constant*
        create_separator_and_add_constraints_to_context (
                const tiramisu::expr& loop_upper_bound, int v);

protected:

    /**
      * Initialize a computation.
      * This is a private function that should not be called explicitly
      * by users.
      */
    void init_computation(std::string iteration_space_str,
                          tiramisu::function *fct,
                          const tiramisu::expr &e,
                          bool schedule_this_computation,
                          tiramisu::primitive_t t);

    /**
     * Dummy constructor for derived classes.
     */
    computation();

public:

    /**
      * Constructor for computations.
      *
      * \p iteration_domain_str is a string that represents the iteration
      * domain of the computation.  The iteration domain should be written
      * in the ISL format (http://barvinok.gforge.inria.fr/barvinok.pdf Section 1.2.1).
      *
      * The iteration domain of a statement is a set that contains
      * all of the execution instances of the statement (a statement in a
      * loop has an execution instance for each loop iteration in which
      * it executes). Each execution instance of a statement in a loop
      * nest is uniquely represented by an identifier and a tuple of
      * integers  (typically,  the  values  of  the  outer  loop  iterators).
      *
      * For example, the iteration space of the statement S0 in the following
      * loop nest
      * for (i=0; i<2; i++)
      *   for (j=0; j<3; j++)
      *      S0;
      *
      * is {S0(0,0), S0(0,1), S0(0,2), S0(1,0), S0(1,1), S0(1,2)}
      *
      * S0(0,0) is the execution instance of S0 in the iteration (0,0).
      *
      * The previous set of integer tuples can be compactly described
      * by affine constraints as follows
      *
      * {S0(i,j): 0<=i<2 and 0<=j<3}
      *
      * In general, the loop nest
      *
      * for (i=0; i<N; i++)
      *   for (j=0; j<M; j++)
      *      S0;
      *
      * has the following iteration domain
      *
      * {S0(i,j): 0<=i<N and 0<=j<M}
      *
      * This should be read as: the set of points (i,j) such that
      * 0<=i<N and 0<=j<M.
      *
      * The name of the computation in the iteration domain should not
      * start with _ (an underscore).  Names starting with _ are reserved
      * names.
      *
      * \p e is the expression computed by the computation.
      *
      * \p schedule_this_computation should be set to true if the computation
      * is supposed to be schedule and code is supposed to be generated from
      * the computation.  Set it to false if you just want to use the
      * computation to represent a buffer (that is passed as an argument
      * to the function) and you do not intend to generate code for the
      * computation.
      *
      * \p t is the type of the computation, i.e. the type of the expression
      * computed by the computation. Example of types include (p_uint8,
      * p_uint16, p_uint32, ...).
      *
      * \p fct is a pointer to the Tiramisu function where this computation
      * should be added.
      */
    computation(std::string iteration_domain_str, tiramisu::expr e,
                bool schedule_this_computation, tiramisu::primitive_t t,
                tiramisu::function *fct);

    /**
      * Return the access function of the computation.
      */
    isl_map *get_access_relation() const;

    /**
      * Return the access function of the computation after transforming
      * it to the time-processor domain.
      * The domain of the access function is transformed to the
      * time-processor domain using the schedule, and then the transformed
      * access function is returned.
      */
      isl_map *get_access_relation_adapted_to_time_processor_domain() const;

      /**
        * Return the context of the computations.
        */
      isl_ctx *get_ctx() const;

      /**
       * Get the data type of the computation.
       */
      tiramisu::primitive_t get_data_type() const;

      /**
       * Return the Tiramisu expression associated with the computation.
       */
      const tiramisu::expr &get_expr() const;

      /**
        * Return the function where the computation is declared.
        */
      tiramisu::function *get_function() const;

      /**
        * Return the Halide statement that assigns the computation to a buffer location.
        */
      Halide::Internal::Stmt get_halide_stmt() const;

      /**
        * Return vector of isl_ast_expr representing the indices of the array where
        * the computation will be stored.
        */
      std::vector<isl_ast_expr *> &get_index_expr();

      /**
        * Return the iteration domain of the computation.
        * In this representation, the order of execution of computations
        * is not specified, the computations are also not mapped to memory.
        */
      isl_set *get_iteration_domain() const;

      /**
        * Return the name of the computation.
        */
      const std::string &get_name() const;

      /**
       * Get the number of dimensions of the iteration
       * domain of the computation.
       */
      int get_n_dimensions();

      /**
        * Return the schedule of the computation.
        */
      isl_map *get_schedule() const;

      /**
        * Return the time-processor domain of the computation.
        * In this representation, the logical time of execution and the
        * processor where the computation will be executed are both
        * specified.
        */
      isl_set *get_time_processor_domain() const;

     /**
       * Return if this computation represents a let statement.
       *
       * Let statements should be treated differently because:
       * - A let statement does not have/need an access function because
       * it writes directly to a scalar.
       * - If the backend is Halide:
       *      - During Halide code generation a Halide let statement
       *      should be created instead of an assignment statement.
       *      - When targeting Halide, let statements should be created
       *      after their body is created, because the body is an argument
       *      needed for the creation of the let statement.
       */
    bool is_let_stmt() const;

    /**
      * Return true if the this computation is supposed to be scheduled
      * by Tiramisu.
      */
    bool should_schedule_this_computation() const;

    /**
     * Set the access function of the computation.
     *
     * The access function is a relation from computations to buffer locations.
     * \p access_str is a string that represents the relation (in ISL format,
     * http://isl.gforge.inria.fr/user.html#Sets-and-Relations).
     */
    // @{
    void set_access(std::string access_str);
    void set_access(isl_map *access);
    // @}

    /**
     * Set the expression of the computation.
     */
    void set_expression(const tiramisu::expr &e);

    /**
     * Set an identity schedule for the computation.
     *
     * This identity schedule is an identity relation created from the iteration
     * domain.
     */
    void set_identity_schedule_based_on_iteration_domain();

    /**
     * Set the name of the computation.
     */
    void set_name(const std::string n);

    /**
      * Set the schedule indicated by \p map.
      *
      * \p map is a string that represents a mapping from the iteration domain
      * to the time-processor domain (the ISL format to represent maps is
      * documented in http://barvinok.gforge.inria.fr/barvinok.pdf in Sec 1.2.2).
      *
      * The schedule is a map from the iteration domain to a time processor
      * domain. The same name of space should be used for both the range
      * and the domain of the schedule.
      *
      * In the time-processor domain, static and dynamic dimensions are
      * interleaved: static, dynamic, static, dynamic, ...
      * Static dimensions are used to order statements within a given block
      * of statements in a given loop level while dynamic dimensions represent
      * the actual loop levels.
      * For example, the computations c0 and c1 in the following loop nest
      *
      * for (i=0; i<N: i++)
      *   for (j=0; j<N; j++)
      *   {
      *     c0;
      *     c1;
      *   }
      *
      * have the following representations in the iteration domain
      *
      * {c0[i,j]: 0<=i<N and 0<=j<N}
      * {c1[i,j]: 0<=i<N and 0<=j<N}
      *
      * and the following representation in the time-processor domain
      *
      * {c0[0,i,0,j,0]: 0<=i<N and 0<=j<N}
      * {c1[0,i,0,j,1]: 0<=i<N and 0<=j<N}
      *
      * The first dimension (dimension 0) in the time-processor
      * representation (the leftmost dimension) is a static dimension,
      * the second dimension (dimension 1) is a dynamic dimension that
      * represents the loop level i, ..., the forth dimension is a dynamic
      * dimension that represents the loop level j and the last dimension
      * (dimension 4) is a static dimension and allows the ordering of
      * c1 after c0 in the loop nest.
      *
      * To transform the previous iteration domain representation to the
      * time-processor domain representation, the following schedule should
      * be used:
      *
      * {c0[i,j]->c0[0,i,0,j,0]: 0<=i<N and 0<=j<N}
      * {c1[i,j]->c1[0,i,0,j,1]: 0<=i<N and 0<=j<N}
      *
      */
    // @{
    void set_schedule(isl_map *map);
    void set_schedule(std::string map_str);
    // @}

    /**
     * Compare two computations.
     *
     * Two computations are considered to be equal if they have the
     * same name.
     */
    bool operator==(tiramisu::computation comp1);

    /**
     * Access operator: C0(i,j) represents an access to
     * the element (i,j) of the computation C0.
     * C0(i,j) represents the value computed by the computation
     * C0(i,j)
     *
     */
    template<typename... Args>
    tiramisu::expr operator()(Args... args)
    {
        std::vector<tiramisu::expr> access_expressions{std::forward<Args>(args)...};
        return tiramisu::expr(tiramisu::o_access,
                          tiramisu::expr(this->get_name()),
                          access_expressions,
                          this->get_data_type());
    }

    /**
      * Schedule this computation to run after the computation \p comp
      * within the loop level \p level.  The outermost loop level is 0.
      *
      * For example assuming we have the two computations
      *
      *     {S0[i,j]: 0<=i<N and 0<=j<N} and {S1[i,j]: 0<=i<N and 0<=j<N}
      *
      * In order to make S1 run after S0 in the i loop, one should use
      *
      *     S1.after(S0,0)
      *
      * which means: S1 is after S0 at the loop level 0 (which is i).
      *
      * The corresponding code is
      *
      *     for (i=0; i<N; i++)
      *     {
      *         for (j=0; j<N; j++)
      *             S0;
      *         for (j=0; j<N; j++)
      *             S1;
      *     }
      *
      * S1.after(S0,1)
      *
      * means: S1 is after S0 at the loop level 1 (which is j) and would yield
      * the following code
      *
      * for (i=0; i<N; i++)
      *   for (j=0; j<N; j++)
      *   {
      *     S0;
      *     S1;
      *   }
      *
      * S1.after(S0, computation::root_dimension)
      * means S1 is after S0 at the main program level and would yield
      * the following code
      *
      * for (i=0; i<N; i++)
      *   for (j=0; j<N; j++)
      *     S0;
      * for (i=0; i<N; i++)
      *   for (j=0; j<N; j++)
      *     S1;
      */
    void after(computation &comp, int level);

    /**
      * Schedule this computation to run before the computation \p comp
      * at the loop level \p L.  The outermost loop level is 0.
      *
      * Use computation::root_dimension to indicate the root dimension
      * (i.e. the outermost processor-time dimension).
      *
      * The outermost loop has a loop level equal to zero.
      */
    void before(computation &comp, int L);

    /**
      * Schedule this computation to run first at the loop level
      * \p L.
      *
      * Calling this function is not needed any more. Calling
      * after() alone is sufficient now. A computation that is
      * not ordered after other computations is considered to be
      * the first automatically.
      *
      */
    void first(int L);

    /**
     * Fuse the loop over this computation with the loop over the
     * computations passed as arguments. Fuse at the loop level
     * \p lev.
     *
     * For example, assuming we have the following computations
     *
     * {S0[i,j]: 0<=i<N and 0<=j<N}, {S1[i,j]: 0<=i<N and 0<=j<N}
     * and {S2[i,j]: 0<=i<N and 0<=j<N}.
     *
     * With the default schedule, these computations would be equivalent
     * to the following loops nests
     *
     * for (i=0; i<N; i++)
     *   for (j=0; j<N; j++)
     *     S0;
     *
     * for (i=0; i<N; i++)
     *   for (j=0; j<N; j++)
     *     S1;
     *
     * for (i=0; i<N; i++)
     *   for (j=0; j<N; j++)
     *     S2;
     *
     * S2.fuse_after(1, S0, S1);
     *
     * would result in fusing S2 with S0 and S1 at loop level 1,
     * the resulting code would look like
     *
     * for (i=0; i<N; i++)
     *   for (j=0; j<N; j++)
     *   {
     *     S0;
     *     S1;
     *     S2;
     *   }
     *
     * S2.fuse_after(0, S0, S1);
     *
     * would result in the following code
     *
     * for (i=0; i<N; i++)
     * {
     *   for (j=0; j<N; j++)
     *     S0;
     *   for (j=0; j<N; j++)
     *     S1;
     *   for (j=0; j<N; j++)
     *     S2;
     * }
     *
     */
    template<typename... Args> void fuse_after(int lev, Args... args)
    {
        std::vector<tiramisu::computation> computations{std::forward<Args>(args)...};

        if (computations.size()>0)
        {
            this->after(computations[0], lev);

            for (int i=0; i<computations.size()-1; i++)
                computations[i].after(computations[i+1], lev);
        }
    }

    /**
     * Interchange (swap) the two loop levels \p L0 and \p L1.
     */
    void interchange(int L0, int L1);

    /**
      * Split the loop level \p L0 of the iteration space into two
      * new loop levels.
      *
      * The outermost loop level is 0.
      *
      * \p sizeX is the extent (size) of the inner loop created after
      * splitting.
      */
     void split(int L0, int sizeX);

    /**
      * Tag the loop level \p L0 and \p L1 to be mapped to GPU.
      *
      * The outermost loop level is 0.
      */
    void tag_gpu_levels(int L0, int L1);

    /**
      * Tag the loop level \p L to be parallelized.
      *
      * The outermost loop level is 0.
      *
      */
    void tag_parallel_level(int L);

    /**
      * Tag the loop level \p L to be vectorized.  The outermost loop level
      * is 0.
      *
      * The user can only tag loop levels that have constant extent.
      * If a loop level does not have a constant extent, the user
      * should call .vectorize() command instead or he can call
      * separate() and split() manually.
      */
    void tag_vector_level(int L);

    /**
      * Tag the loop level \p L to be unrolled.
      * The outermost loop level is 0.
      *
      * The user can only tag loop levels that have constant extent.
      */
    void tag_unroll_level(int L);

    /**
      * Tile the two loop levels \p L0 and \p L1 with rectangular
      * tiling.  \p sizeX and \p sizeY represent the tile size.
      * \p L0 and \p L1 should be two consecutive loop levels
      * (i.e., \p L0 = \p L1 + 1) and they should satisfy
      * \p L0 > \p L1.
      */
    void tile(int L0, int L1, int sizeX, int sizeY);

    /**
     * Unroll the loop level \p L with an unrolling factor \p fac
     * and assume that the upper bound of the loop level \p L is
     * \p loop_upper_bound.
     *
     * The difference between this function and the function
     * tag_unroll_level(int L) is that this function separates
     * the iteration domain into full and partial iteration
     * domains for unrolling first and then it calls
     * tag_unroll_level(int L).
     * tag_unroll_level(int L) only tags a dimension to
     * be unrolled, it does not modify the tagged dimension.
     *
     * This function separates the iteration domain into two iteration
     * domains, a full iteration domain and a partial iteration domain.
     * The full iteration domain has an upper bound that is multiple of
     * \p fac while the other does not.
     * The full iteration domain is then split by \p fac and the inner loop
     * (which should have a constant extent equal to \p fac) is tagged as
     * a unrolled loop.
     *
     * Let us assume the following loop (a loop represents and iteration
     * domain)
     *
     * for (i=0; i<N; i++)
     *   for (j=0; j<23; j++)
     *     S0;
     *
     * To unroll the j loop with an unrolling factor of 4, one should call
     *
     *      S0.unroll(1, 4, 23);
     *
     * The loop (iteration domain) is first separated into the following
     * two loops
     *
     * for (int i=0; i<20; i++)
     *   S0;
     *
     * for (int i=20; i<23; i++)
     *   S0;
     *
     * The full loop is then split by 4
     *
     * for (int i1=0; i1<20/4; i1++)
     *   for (int i2=0; i2<4; i2++)
     *      S0;
     *
     * for (int i=20; i<23; i++)
     *   S0;
     *
     * the i2 loop is then tagged to be unrolled.
     *
     */
    void unroll(int L, int fac, tiramisu::expr loop_upper_bound);

    /**
     * Vectorize the loop level \p L.  Use the vector length \p v
     * and assume that the upper bound of the loop level \p L is
     * \p loop_upper_bound.
     *
     * The difference between this function and the function
     * tag_vector_level(int L) is that this function
     * prepares the iteration domain for vectorization first
     * and then it calls tag_vector_level(int L).
     * tag_vector_level(int L) only tags a dimension to
     * be vectorized, it does not change the tagged dimension.
     *
     * This function will separate the iteration domain into two iteration
     * domains, a full iteration domain and a partial iteration domain.
     * The full iteration domain has an upper bound that is multiple of
     * \p v while the other does not.
     * The full iteration domain is then split by \p v and the inner loop
     * (which should have a constant extent equal to \p v) is tagged as
     * a vector loop.
     *
     * Let us assume the following loop (a loop represents and iteration
     * domain)
     *
     * for (i=0; i<N; i++)
     *   for (j=0; j<23; j++)
     *     S0;
     *
     * To vectorize the j loop with a vector length 4, one should call
     *
     *      S0.vectorize(1, 4, 23);
     *
     * The loop (iteration domain) is first separated into the following
     * two loops
     *
     * for (int i=0; i<20; i++)
     *   S0;
     *
     * for (int i=20; i<23; i++)
     *   S0;
     *
     * The full loop is then split by 4
     *
     * for (int i1=0; i1<20/4; i1++)
     *   for (int i2=0; i2<4; i2++)
     *      S0;
     *
     * for (int i=20; i<23; i++)
     *   S0;
     *
     * the i2 loop is then tagged to be vectorized.
     *
     */
    void vectorize(int L, int v, tiramisu::expr loop_upper_bound);

    /**
      * Bind the computation to a buffer.
      * i.e. create a one-to-one data mapping between the computation
      * the buffer.
      */
    void bind_to(buffer *buff);

    /*
      * Create a Halide statement that assigns the computations to the memory
      * buffer and location specified by the access function.
      */
     void create_halide_assignment();

     /**
      * Generate an identity schedule for the computation.
      *
      * This identity schedule is an identity relation created from the iteration
      * domain.
      */
     isl_map *gen_identity_schedule_for_iteration_domain();

     /**
      * Generate an identity schedule for the computation.
      *
      * This identity schedule is an identity relation created from the
      * time-processor domain.
      */
     isl_map *gen_identity_schedule_for_time_space_domain();

   /**
     * Generate the time-processor domain of the computation.
     *
     * In this representation, the logical time of execution and the
     * processor where the computation will be executed are both
     * specified.  The memory location where computations will be
     * stored in memory is not specified at the level.
     */
    void gen_time_processor_domain();

    /**
     * Mark this statement as a let statement.
     */
    void mark_as_let_statement();

    /**
      * root_dimension is a number used to specify the dimension level
      * known as root.
      * The root dimension level is the outermost level.  It is the level
      * outside any loop nest.  Loop level 0 is the level of the first loop
      * (outermost loop), loop 1 is the level of following inner loop, ...
      *
      * Where is this number used ?
      *
      * These numbers are used in the helper functions used for scheduling
      * (such as after(), before(), ...).
      * For example, c0.after(c1) indicates that the computation c0 should
      * be executed after the computation c1.
      * Since the two computations c0 and c1 are usually nested in a loop,
      * we need to specify at which loop level c0 is after c1. This is where
      * we need to specify the loop level numbers.
      * Here is an example.  Suppose that the two computations c0 and c1
      * have the following iteration domains
      * {c0[i,j]: 0<=i<N and 0<=j<N} and {c1[i,j]: 0<=i<N and 0<=j<N}.
      *
      * When code is generated for the two computations, two loop nests
      * are generated.  When scheduling c0 after c1 using the after function,
      * the user can choose one among three possibilities in specifying at
      * which level c0 is after c1.
      *
      * - c0.after(c1, computation::root_dimension) would create a schedule
      * that generates the following code
      *
      * for (i=0; i<N; i++)
      *     for (j=0; j<N; j++)
      *         c1;
      * for (i=0; i<N; i++)
      *     for (j=0; j<N; j++)
      *         c0;
      *
      * - c0.after(c1, 0) would create a schedule that generates the
      * following code
      *
      * for (i=0; i<N; i++) {
      *     for (j=0; j<N; j++)
      *         c1;
      *     for (j=0; j<N; j++)
      *         c0;
      * }
      *
      * This means that c0 is after c1 starting from loop level 0,
      * (before the loop level 0, c0 and c1 have the same order).
      *
      * - c0.after(c1, 1) would create a schedule that generates the
      * following code
      *
      * for (i=0; i<N; i++)
      *     for (j=0; j<N; j++) {
      *         c1;
      *         c0;
      *     }
      *
      * This means that c0 is after c1 starting from loop level 1,
      * (before the loop level 1, c0 and c1 have the same order).
      */
    const static int root_dimension = -1;

    /**
     * Let statements that should be computed before this computation.
     *
     * This is mainly useful when this computation consumes values
     * computed in let statements, so those let statements should
     * be executed before this computation.
     */
    tiramisu::computation *statements_to_compute_before_me;

    /**
      * Dump the iteration domain of the computation.
      * This is useful for debugging.
      */
    void dump_iteration_domain() const;

    /**
      * Dump the schedule of the computation.
      * This is mainly useful for debugging.
      * The schedule is a relation between the iteration space and a
      * time space.  The relation provides a logical date of execution for
      * each point in the iteration space.
      * The schedule needs first to be set before calling this function.
      */
    void dump_schedule() const;

    /**
      * Dump (on stdout) the computation (dump most of the fields of the
      * computation class).
      * This is mainly useful for debugging.
      */
    void dump() const;
};


/**
  * A class that represents loop invariants.
  *
  * An object of the invariant class can be an expression, a symbolic constant
  * or a variable that is invariant to all the loops of the function.
  */
class constant: public computation
{
public:

    /**
      * Create a constant where \p param_name is the name of
      * the constant that will hold the value of the constant.
      *
      * \p param_expr is the expression that defines the value
      * of the constant.
      *
      * \p t indicates the type of the constant.
      *
      * \p function_wide should be set to true if the constant is
      * defined at the entry of the function and is visible to all
      * the computations.
      * If function_wide is set to true, then the constant is an
      * invariant to the whole function where it is declared.
      *
      * \p with_computation, should be set only if function_wide
      * is false, i.e. if the constant is not function wide.
      * In such a case the user should indicate where the
      * constant should be assigned.
      * \p with_computation indicates that the assignment should
      * be in the loop nest that computes the computation indicated by
      * \p with_computation at the loop level indicated
      * by \p at_loop_level.
      * The root level (i.e. the level outer than any other loop level)
      * is computation::root_dimension.
      * 0 represents the first loop level and 1 represents the second
      * loop level, ...
      *
      * \p func is the function in which the constant is defined.
      */
    constant(std::string param_name, const tiramisu::expr &param_expr,
             tiramisu::primitive_t t,
             bool function_wide,
             tiramisu::computation *with_computation,
             int at_loop_level,
             tiramisu::function *func);

    /**
      * Dump the invariant on standard output (dump most of the fields of
      * the invariant class).
      * This is mainly useful for debugging.
      * If \p exhaustive is set to true, all the fields of the invaraint
      * class are printed.  This is useful to find potential initialization
      * problems.
      */
    void dump(bool exhaustive) const;
};

// Halide IR specific functions

void halide_stmt_dump(Halide::Internal::Stmt s);

Halide::Internal::Stmt lower_halide_pipeline(const Halide::Target &t, Halide::Internal::Stmt s);

int loop_level_into_dynamic_dimension(int level);
int loop_level_into_static_dimension(int level);
/**
 * TODO code cleaning:
 * - Go to the tutorials, add a small explanation about how Tiramisu should work in general.
 * - Add two pages explaining how one should use Tiramisu,
 *
 * - Have documentation on header files only,
 * - Order the functions in the class computations (get functions then update functions ordered in alphabetical order),
 * - Clean/document expr.h and type.h
 */

}

#endif
